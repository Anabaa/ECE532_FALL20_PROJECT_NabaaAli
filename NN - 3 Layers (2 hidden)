# This code shall be executed along with the loading data, preprocessing codes and minmax scale codes 
## Neural network with 2 hidden layers 
from random import randint
# split the data into 7 sets 
k =10000
d =  7
def logsig(_x):
        return 1/(1+np.exp(-_x))  
y_filter[y_filter == -1] = 0
#step size
alpha = 1e-2
L = 50 # number of epochs
# l_2 regularizer 
lam = 1e-3
M = 11 #number of hidden nodes in layer 1 
R = 11 # number of hidden nodes in layer 2 
V = np.random.randn(R+1, 1); 
m = np.random.randn(M+1,R+1)
W = np.random.randn(12, M);
avg_train_err_each_epoch = []
avg_eval_err_each_epoch = []
for epoch in range(L):
    err_7sets_train_2lay_list = []
    err_7sets_eval_2lay_list = []
    for s in range (d):
        start_index = (s)*k
        end_index = (s+1)*k 
        x_unq_train = np.vstack((X_scale[0:start_index,:],X_scale[end_index:-1,:]))
        y_unq_train = np.vstack((y_filter[0:start_index,:],y_filter[end_index:-1,:]))
        rows_train = x_unq_train.shape[0]
        x_unq_eval = X_scale[start_index:end_index,:]
        y_unq_eval = y_filter[start_index:end_index,:]
        rows_eval = x_unq_eval.shape[0]
        # train 
        ind = np.random.permutation(rows_train) 
        for i in ind:
            #forward
            H = logsig(np.hstack((np.ones((1,1)), x_unq_train[[i],:]@W))) 
            K = logsig(H@m)
            Yhat = logsig(K@V)
            # Backpropagate and update the weights 
            delta = (Yhat-y_unq_train[i,:])*Yhat*(1-Yhat)
            Vnew = V-alpha*K.T@delta -alpha*(2/rows_train)*lam*V
            gamma = delta@V.T*K*(1-K)
            mnew = m - alpha*H.T@gamma -alpha*(2/rows_train)*lam*m
            beta = gamma@m[1:,:].T*H[:,1:]*(1-H[:,1:])
            Wnew = W - alpha*x_unq_train[[i],:].T@beta  -alpha*(2/rows_train)*lam*W
            V = Vnew
            m = mnew
            W = Wnew
        # Calculate training error for this set 
        H = logsig( np.hstack((np.ones((rows_train,1)), x_unq_train@W)))
        K = logsig(H@m)
        Yhat = logsig(K@V)
        #print(Yhat)
        y_pred_train = np.zeros((rows_train,1))
        for t in range (rows_train):
            if Yhat[t,:] < 0.5 :
                y_pred_train[t] = 0
            else:
                y_pred_train[t] = 1
        err_rate = np.mean(y_pred_train!= y_unq_train)
        # Calculate evaluation error for this set 
        H_eval = logsig( np.hstack((np.ones((rows_eval,1)), x_unq_eval@W)))
        K_eval = logsig(H_eval@m)
        Yhat_eval = logsig(K_eval @V)
        y_pred_eval = np.zeros((rows_eval,1))
        for j in range (rows_eval):
            if Yhat_eval[j,:] < 0.5 :
                y_pred_eval[j] = 0
            else:
                y_pred_eval[j] = 1
        err_rate_eval = np.mean(y_pred_eval!= y_unq_eval)
        err_7sets_train_2lay_list.append(err_rate)
        err_7sets_eval_2lay_list.append(err_rate_eval)
    print(epoch)
    # Calculate average training and evaluation error for this epoch 
    avg_err_train =np.mean(err_7sets_train_2lay_list)
    avg_err_eval =np.mean(err_7sets_eval_2lay_list)
    avg_train_err_each_epoch.append(avg_err_train)
    avg_eval_err_each_epoch.append(avg_err_eval)
    print('Average train error after epoch',epoch,':' ,(avg_err_train*100))
    print('Average eval error after epoch',epoch,':' ,(avg_err_eval*100))
# plot the average training error and the average evaluation error versus epochs 
fig = plt.figure()
x = np.arange(1,L+1)
plt.plot(x,avg_eval_err_each_epoch,c='red',label='Evaluation error')
plt.plot(x,avg_train_err_each_epoch,c='k',label='Training error')
plt.xlabel('Number of Epochs')
plt.ylabel('Error %')
plt.legend()
fig.savefig("Trainingvseval_after_each_epoch-2NN.pdf")
